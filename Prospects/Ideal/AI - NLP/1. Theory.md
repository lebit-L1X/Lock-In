### Tokenization, Lemmatization, Stemming
(Source: _Speech and Language Processing_)

In **Speech and Language Processing**, text normalization is a crucial step in preparing text for NLP tasks. It involves converting text into a standardized form, including processes like **tokenization**, **lemmatization**, and **stemming**. **Tokenization** separates words in running text, addressing challenges like multi-word phrases (e.g., "New York") or contractions (e.g., "I’m" → "I am"). It also handles specific cases like hashtags or emoticons, particularly in informal texts like tweets.

**Lemmatization** identifies the base form or lemma of a word, grouping variations like "sang," "sung," and "sing" under "sing." **Stemming** is a simpler approach that removes suffixes from words to derive a root form, without necessarily ensuring the root is a valid word, such as stripping “running” to “run.”

Another important aspect of normalization is **sentence segmentation**, which breaks up text into individual sentences using punctuation cues.

However, text normalization can be especially challenging for languages like **Japanese**, which do not have spaces between words, making word tokenization more difficult.

Additionally, comparing words or strings is facilitated by metrics like **edit distance**, which calculates similarity based on the number of insertions, deletions, or substitutions needed to transform one string into another. This has applications in spelling correction, speech recognition, and other NLP tasks.

### Regex
(Source: _Speech and Language Processing_)

Regex 