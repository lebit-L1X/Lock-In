### Tokenization, Lemmatization, Stemming
(Source: _Speech and Language Processing_)

In **Speech and Language Processing**, text normalization is a crucial step in preparing text for NLP tasks. It involves converting text into a standardized form, including processes like **tokenization**, **lemmatization**, and **stemming**. **Tokenization** separates words in running text, addressing challenges like multi-word phrases (e.g., "New York") or contractions (e.g., "I’m" → "I am"). It also handles specific cases like hashtags or emoticons, particularly in informal texts like tweets.

**Lemmatization** identifies the base form or lemma of a word, grouping variations like "sang," "sung," and "sing" under "sing." **Stemming** is a simpler approach that removes suffixes from words to derive a root form, without necessarily ensuring the root is a valid word, such as stripping “running” to “run.”

Another important aspect of normalization is **sentence segmentation**, which breaks up text into individual sentences using punctuation cues.

However, text normalization can be especially challenging for languages like **Japanese**, which do not have spaces between words, making word tokenization more difficult.

Additionally, comparing words or strings is facilitated by metrics like **edit distance**, which calculates similarity based on the number of insertions, deletions, or substitutions needed to transform one string into another. This has applications in spelling correction, speech recognition, and other NLP tasks.

### Regex
(Source: _Speech and Language Processing_)

Regular expressions (regex) are a powerful tool for text processing, used to search for patterns in strings. They are employed in programming languages, text processing tools like **grep**, and editors like **vim** and **Emacs**. A regex specifies a pattern to search for within a corpus of text, returning all matching strings. Regex can be used to find all matches in a document or just the first one. There are various types of regex, and **extended regular expressions** are typically used, though different parsers may have slight variations. Testing regex online can help explore and refine patterns.

<a href = "https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_expressions">Learn Regex </a> 

Chatbots like **ELIZA** use regex and grouping to match patterns in user input and generate responses. Regex allows the bot to identify key phrases, while grouping captures dynamic parts of the input for reuse in replies. For instance, a pattern like `"I feel (.*)"` matches input such as "I feel sad," capturing "sad" for use in a response like "Why do you feel sad?" This approach enables ELIZA to simulate conversational depth by dynamically tailoring its replies to user input.

### Word Processing

Words in a corpus can be defined differently depending on the task, such as whether punctuation is counted. For example, the Brown Corpus contains diverse written English texts, while the Switchboard Corpus includes spoken American English, introducing complexities like disfluencies. Disfluencies, such as fragments (e.g., "main-") and filled pauses (e.g., "uh" and "um"), may be included or excluded based on the application. While disfluencies are often stripped for tasks like transcription, they can aid speech recognition by signaling clause restarts and even assist in speaker identification. For example, "uh" and "um" may have distinct roles, as shown in the utterance: _"I do uh main- mainly business data processing."_

Word types and word instances are two ways of describing words in a corpus. Word types refer to the number of distinct, unique words in the text, where each unique word is counted only once, regardless of how many times it appears. For example, in the sentence _"The cat chased the rat,"_ the word types are "The," "cat," "chased," and "rat," resulting in four unique words. In contrast, word instances refer to the total number of words in the text, including all repetitions. In the same sentence, the word instances total six because "The" appears twice, and the other words appear once each. This distinction is important for analyzing vocabulary diversity (types) versus word frequency (instances).

Herdan's or Heaps' Law describes the relationship between the vocabulary size and the total word count in a corpus. This relationship is expressed as `|V| = kN^β`, where k is a constant, and 0<β<10. For large English corpora, β typically ranges between 0.67 and 0.75, indicating that vocabulary grows faster than the square root of the text's length. Words in a corpus can be categorized as wordforms and lemmas. Wordforms refer to specific inflected or derived versions of a word, such as "cat" and "cats." Lemmas, on the other hand, represent the base form shared by related wordforms, such as "cat" being the lemma for both "cat" and "cats." While lemmatization is essential for morphologically complex languages, wordforms are often sufficient for most English language tasks.

### Corpura

Language variation is crucial when developing natural language processing (NLP) models, as texts are produced by specific speakers or writers, in particular dialects, languages, and contexts. The world has over 7,000 languages, and NLP tools must be tested across multiple languages, especially those with distinct features. For example, African American Vernacular English (AAVE) has unique features that influence word segmentation, such as the use of constructions like "iont" for "I don’t" and "talmbout" for "talking about." Additionally, many texts involve code-switching, where multiple languages are used within a single communicative act, such as the mixture of English, Spanish, and Hindi in these examples: "Por primera vez veo a @username actually being hateful! it was beautiful:)" and "dost tha or ra- hega ... dont wory ... but dherya rakhe." Text also varies by genre, whether it is from newswire, fiction, scientific articles, or spoken genres like telephone conversations or medical interviews. Demographics like age, gender, race, and socioeconomic class further influence the language used. Moreover, language changes over time, and texts from different historical periods can reflect these changes. Therefore, when developing computational models for language processing, it's important to consider who produced the language, in what context, and for what purpose. The best way to capture these details is for the corpus creator to build a datasheet or data statement for each corpus.

The datasheet specifies properties of a dataset such as:
- Motivation: Why was the corpus collected, by whom, and who funded it?
- Situation: When and in what situation was the text written/spoken? For example, was there a task? Was the language originally spoken conversation, edited text, social media communication, monologue vs. dialogue?
- Language variety: What language (including dialect/region) was the corpus in?
- Speaker demographics: What was, e.g., the age or gender of the text’s authors?
- Collection process: How big is the data? If it is a subsample how was it sampled? Was the data collected with consent? How was the data pre-processed, and what metadata is available?
- Annotation process: What are the annotations, what are the demographics of the annotators, how were they trained, how was the data annotated?

The context of the language used is important in NLP because language is shaped by various factors such as the speaker's or writer's background, the setting, and the purpose of communication. These factors affect how words and phrases are constructed and understood.

